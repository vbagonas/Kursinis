{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcijų išrinkimas (naudojant MPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tree_sitter import Language, Parser\n",
    "from socket import gethostname\n",
    "\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "first_folder = \"extracted data\"\n",
    "second_folder = \"V5\"\n",
    "GRAMMAR_DIR = SCRIPT_DIR / \"tree-sitter\"\n",
    "INPUT_PARQUET = SCRIPT_DIR / first_folder / second_folder / f\"accepted_submissions_{second_folder}.parquet\"\n",
    "OUTPUT_PARQUET_FINAL = SCRIPT_DIR / first_folder / second_folder / f\"extracted_functions_nocom_{second_folder}.parquet\"\n",
    "OUTPUT_PARQUET_FINAL.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LANGUAGE_GRAMMARS = {\n",
    "    \"Python\": \"tree-sitter-python\",\n",
    "    \"C++\": \"tree-sitter-cpp\",\n",
    "    \"C\": \"tree-sitter-c\",\n",
    "    \"Java\": \"tree-sitter-java\",\n",
    "    \"C#\": \"tree-sitter-c-sharp\",\n",
    "}\n",
    "\n",
    "LANGUAGE_SYMBOLS = {\n",
    "    \"Python\": \"python\",\n",
    "    \"C++\": \"cpp\",\n",
    "    \"C\": \"c\",\n",
    "    \"Java\": \"java\",\n",
    "    \"C#\": \"c_sharp\",\n",
    "}\n",
    "\n",
    "LANGUAGE_FUNCTION_NODES = {\n",
    "    \"Python\": {\"function_definition\"},\n",
    "    \"C\": {\"function_definition\"},\n",
    "    \"C++\": {\"function_definition\"},\n",
    "    \"Java\": {\"method_declaration\", \"constructor_declaration\"},\n",
    "    \"C#\": {\"method_declaration\", \"constructor_declaration\"},\n",
    "}\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "hostname = gethostname()\n",
    "print(f\"[Rank {rank}] running on {hostname} (total ranks = {size})\")\n",
    "\n",
    "so_path = SCRIPT_DIR / \"build_my_languages.so\"\n",
    "if rank == 0 and not so_path.exists():\n",
    "    Language.build_library(\n",
    "        str(so_path),\n",
    "        [str(GRAMMAR_DIR / folder) for folder in LANGUAGE_GRAMMARS.values()]\n",
    "    )\n",
    "comm.Barrier()\n",
    "\n",
    "PARSERS = {}\n",
    "for lang, folder in LANGUAGE_GRAMMARS.items():\n",
    "    lang_symbol = LANGUAGE_SYMBOLS[lang]\n",
    "    lang_obj = Language(str(so_path), lang_symbol)\n",
    "    parser = Parser()\n",
    "    parser.set_language(lang_obj)\n",
    "    PARSERS[lang] = parser\n",
    "\n",
    "def find_identifier(node):\n",
    "    if node.type == \"identifier\":\n",
    "        return node\n",
    "    for child in node.children:\n",
    "        result = find_identifier(child)\n",
    "        if result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "def extract_functions(code, lang):\n",
    "    parser = PARSERS[lang]\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root = tree.root_node\n",
    "    target_kinds = LANGUAGE_FUNCTION_NODES[lang]\n",
    "    functions = []\n",
    "\n",
    "    def walk(node):\n",
    "        if node.type in target_kinds:\n",
    "            text = code[node.start_byte:node.end_byte]\n",
    "            name_node = find_identifier(node)\n",
    "            name = code[name_node.start_byte:name_node.end_byte] if name_node else None\n",
    "            if name and name.lower().strip(\"_\") == \"main\":\n",
    "                return\n",
    "            functions.append((name, text))\n",
    "        for child in node.children:\n",
    "            walk(child)\n",
    "\n",
    "    walk(root)\n",
    "    return functions\n",
    "\n",
    "def remove_comments(code, lang):\n",
    "    parser = PARSERS[lang]\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    comment_spans = []\n",
    "\n",
    "    def collect_comments(node):\n",
    "        if node.type == \"comment\":\n",
    "            comment_spans.append((node.start_byte, node.end_byte))\n",
    "        for child in node.children:\n",
    "            collect_comments(child)\n",
    "\n",
    "    collect_comments(tree.root_node)\n",
    "    comment_spans.sort(reverse=True)\n",
    "    code_bytes = bytearray(code, \"utf8\")\n",
    "    for start, end in comment_spans:\n",
    "        del code_bytes[start:end]\n",
    "    return code_bytes.decode(\"utf8\", errors=\"ignore\")\n",
    "\n",
    "if rank == 0:\n",
    "    df_raw = pd.read_parquet(INPUT_PARQUET)\n",
    "    chunks = [df_raw.iloc[i::size] for i in range(size)]\n",
    "else:\n",
    "    chunks = None\n",
    "\n",
    "df_local = comm.scatter(chunks, root=0)\n",
    "\n",
    "checkpoint_file = OUTPUT_PARQUET_FINAL.parent / f\"checkpoint_rank_{rank}.parquet\"\n",
    "output_file = OUTPUT_PARQUET_FINAL.parent / f\"extracted_functions_rank_{rank}.parquet\"\n",
    "processed_ids_file = OUTPUT_PARQUET_FINAL.parent / f\"processed_ids_rank_{rank}.txt\"\n",
    "\n",
    "records = []\n",
    "total_functions = 0\n",
    "processed_batch_ids = set()\n",
    "lines_processed = 0\n",
    "\n",
    "for i, row in df_local.iterrows():\n",
    "    lines_processed += 1\n",
    "    lang = row[\"language\"]\n",
    "    file_path = Path(row[\"file_path\"])\n",
    "    try:\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "\n",
    "        code = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        clean_code = remove_comments(code, lang)\n",
    "        funcs = extract_functions(clean_code, lang)\n",
    "\n",
    "        total_functions += len(funcs)\n",
    "        for func_name, func_code in funcs:\n",
    "            records.append({\n",
    "                \"submission_id\": row[\"submission_id\"],\n",
    "                \"language\": lang,\n",
    "                \"file_path\": str(file_path),\n",
    "                \"function_name\": func_name,\n",
    "                \"function_code\": func_code\n",
    "            })\n",
    "        processed_batch_ids.add(str(row[\"submission_id\"]))\n",
    "\n",
    "        if i % 100 == 0 and records:\n",
    "            df_new = pd.DataFrame(records)\n",
    "            if checkpoint_file.exists():\n",
    "                df_existing = pd.read_parquet(checkpoint_file)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "            else:\n",
    "                df_combined = df_new\n",
    "            df_combined.to_parquet(checkpoint_file, index=False)\n",
    "            records = []\n",
    "\n",
    "            with open(processed_ids_file, \"a\") as f:\n",
    "                for pid in processed_batch_ids:\n",
    "                    f.write(pid + \"\\n\")\n",
    "            processed_batch_ids.clear()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {rank}] Error in {file_path}: {e}\")\n",
    "\n",
    "if records:\n",
    "    df_new = pd.DataFrame(records)\n",
    "    if checkpoint_file.exists():\n",
    "        df_existing = pd.read_parquet(checkpoint_file)\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = df_new\n",
    "    df_combined.to_parquet(output_file, index=False)\n",
    "    if checkpoint_file.exists():\n",
    "        checkpoint_file.unlink()\n",
    "    with open(processed_ids_file, \"a\") as f:\n",
    "        for pid in processed_batch_ids:\n",
    "            f.write(pid + \"\\n\")\n",
    "\n",
    "comm.Barrier()\n",
    "if rank == 0:\n",
    "    all_files = list(OUTPUT_PARQUET_FINAL.parent.glob(\"extracted_functions_rank_*.parquet\"))\n",
    "    dfs = [pd.read_parquet(f) for f in all_files]\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.to_parquet(OUTPUT_PARQUET_FINAL, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcijų atrinkimas galutiniam duomenų rinkiniui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "first_folder = \"extracted data\"\n",
    "second_folder = \"V5\"\n",
    "GRAMMAR_DIR = SCRIPT_DIR / \"tree-sitter\"\n",
    "INPUT_PARQUET = SCRIPT_DIR / first_folder / second_folder / f\"accepted_submissions_{second_folder}.parquet\"\n",
    "OUTPUT_PARQUET_FINAL = SCRIPT_DIR / first_folder / second_folder / f\"extracted_functions_nocom_{second_folder}.parquet\"\n",
    "OUTPUT_PARQUET_FINAL.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = OUTPUT_PARQUET_FINAL\n",
    "duomenys = pd.read_parquet(file_path, engine='pyarrow')\n",
    "dupe_mask = duomenys['function_code'].duplicated()\n",
    "duomenys_unique = duomenys.loc[~dupe_mask].reset_index(drop=True)\n",
    "duomenys_unique = duomenys_unique[duomenys_unique['function_code'].str.strip().ne('')]\n",
    "language_counts = duomenys_unique['language'].value_counts()\n",
    "print(language_counts)\n",
    "\n",
    "df_subset = (\n",
    "    duomenys_unique\n",
    "    .groupby('language', group_keys=False)\n",
    "    .apply(lambda grp: grp.sample(n=20000, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_subset.to_parquet('subset_100k.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
