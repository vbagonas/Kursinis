{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programinių kodų filtravimas pagal pasirinktus reikalavimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_DIR = Path().resolve()\n",
    "CODENET_BASE_DIR = SCRIPT_DIR.parent / \"duomenys\" / \"CodeNet\"\n",
    "METADATA_DIR = CODENET_BASE_DIR / \"Project_CodeNet\" / \"metadata\"\n",
    "DATA_DIR = CODENET_BASE_DIR / \"Project_CodeNet\" / \"data\"\n",
    "OUTPUT_PARQUET = SCRIPT_DIR / \"extracted data\" / \"accepted_submissions_temp.parquet\"\n",
    "TARGET_LANGUAGES = {\"Python\", \"C++\", \"C\", \"C#\", \"Java\"}\n",
    "ACCEPTED_STATUS = {\"Accepted\"}\n",
    "\n",
    "def resolve_path(row):\n",
    "    lang_folder = LANGUAGE_FOLDER_MAP.get(row[\"language\"])\n",
    "    prob_id = row[\"problem_id\"]\n",
    "    sub_id = str(row[\"submission_id\"])\n",
    "    ext = \".\" + row[\"filename_ext\"].strip()\n",
    "    return DATA_DIR / prob_id / lang_folder / (sub_id + ext)\n",
    "\n",
    "LANGUAGE_FOLDER_MAP = {\n",
    "    \"C++\": \"C++\",\n",
    "    \"C\": \"C\",\n",
    "    \"C#\": \"C#\",\n",
    "    \"Python\": \"Python\",\n",
    "    \"Java\": \"Java\"\n",
    "}\n",
    "\n",
    "all_records = []\n",
    "print(\"+\")\n",
    "for file in METADATA_DIR.glob(\"*.csv\"):\n",
    "    print(f\"Loading {file.name}\")\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    required = {\"submission_id\", \"problem_id\", \"language\", \"status\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        print(f\"Skipping {file.name} — missing required columns\")\n",
    "        continue\n",
    "    df = df[df[\"language\"].isin(TARGET_LANGUAGES)]\n",
    "    df = df[df[\"status\"].isin(ACCEPTED_STATUS)]\n",
    "    if df.empty:\n",
    "        print(f\"Skipping {file.name} — no accepted rows\")\n",
    "        continue\n",
    "    df[\"file_path\"] = df.apply(resolve_path, axis=1)\n",
    "    df = df[df[\"file_path\"].apply(lambda p: p.exists())]\n",
    "    all_records.append(df)\n",
    "final_df = pd.concat(all_records, ignore_index=True)\n",
    "final_df[\"file_path\"] = final_df[\"file_path\"].astype(str)\n",
    "\n",
    "df_limited = (\n",
    "    final_df.groupby(['problem_id', 'language'], group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min(len(x), 2000), random_state=42))\n",
    ")\n",
    "freq_table = df_limited.groupby(['problem_id', 'language']).size().unstack(fill_value=0)\n",
    "counts = df_limited.groupby(['problem_id', 'language']).size().unstack(fill_value=0)\n",
    "min_required = 100\n",
    "valid_problems = counts[(counts >= min_required).all(axis=1)].index\n",
    "filtered_df = df_limited[df_limited['problem_id'].isin(valid_problems)]\n",
    "columns_to_drop = [\n",
    "    \"user_id\", \"date\", \"original_language\", \n",
    "    \"cpu_time\", \"memory\", \"code_size\", \n",
    "    \"accuracy\", \"status\"\n",
    "]\n",
    "filtered_df = filtered_df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "filtered_df.to_parquet(SCRIPT_DIR / \"extracted data\" / \"accepted_submissions_V5.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
